# neural-networks-scratch

ğŸ” Common Activation Functions
1. Sigmoid
 sigmoid(z) = 1 / (1+e^(âˆ’z))

    Range: (0, 1)

    Used in: Output layer for binary classification

    Problem: Can cause vanishing gradients in deep networks

Visual: S-shaped curve
2. Tanh (Hyperbolic Tangent)
 tanh(z) = (e^(z) âˆ’ e^(âˆ’z)) / (e^(z) + e^(âˆ’z))

    Range: (-1, 1)

    Zero-centered (better than sigmoid in practice)

    Still suffers from vanishing gradients, but less than sigmoid

3. ReLU (Rectified Linear Unit)
 ReLU(z)=maxâ¡(0,z)

    Range: [0, âˆ)

    Very common in hidden layers

    Fast to compute

    Issue: â€œDying ReLUâ€ â€” neurons can get stuck and always output 0