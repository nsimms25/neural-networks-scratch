# neural-networks-scratch

ğŸ” Common Activation Functions
1. Sigmoid
sigmoid(z)=11+eâˆ’z
sigmoid(z)=1+eâˆ’z1â€‹

    Range: (0, 1)

    Used in: Output layer for binary classification

    Problem: Can cause vanishing gradients in deep networks

Visual: S-shaped curve
2. Tanh (Hyperbolic Tangent)
tanh(z)=ezâˆ’eâˆ’zez+eâˆ’z
tanh(z)=ez+eâˆ’zezâˆ’eâˆ’zâ€‹

    Range: (-1, 1)

    Zero-centered (better than sigmoid in practice)

    Still suffers from vanishing gradients, but less than sigmoid

3. ReLU (Rectified Linear Unit)
ReLU(z)=maxâ¡(0,z)
ReLU(z)=max(0,z)

    Range: [0, âˆ)

    Very common in hidden layers

    Fast to compute

    Issue: â€œDying ReLUâ€ â€” neurons can get stuck and always output 0